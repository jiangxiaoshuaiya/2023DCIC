{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "003fd366",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniconda/base/envs/ml/lib/python3.8/site-packages/requests/__init__.py:102: RequestsDependencyWarning: urllib3 (1.26.7) or chardet (5.0.0)/charset_normalizer (2.0.4) doesn't match a supported version!\n",
      "  warnings.warn(\"urllib3 ({}) or chardet ({})/charset_normalizer ({}) doesn't match a supported \"\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import gc\n",
    "import warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from datetime import timedelta\n",
    "\n",
    "\n",
    "import xgboost as xgb\n",
    "from catboost import CatBoostClassifier, CatBoostRegressor\n",
    "from sklearn.model_selection import StratifiedKFold, KFold, GroupKFold\n",
    "from sklearn.metrics import accuracy_score, f1_score, roc_auc_score\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "pd.set_option('display.max_columns',1000)\n",
    "pd.set_option('display.max_rows',1000)\n",
    "\n",
    "\n",
    "trans_info = pd.read_csv('账户交易信息.csv')\n",
    "static_info = pd.read_csv('账户静态信息.csv')\n",
    "session = pd.read_csv('./session.csv')\n",
    "\n",
    "train_set = pd.read_csv('训练集标签.csv')\n",
    "test_set = pd.read_csv('test_dataset.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b2e415e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['zhdh', 'black_flag', 'year', 'month', 'day', 'khjgdh', 'xb', '年龄',\n",
      "       'khrq', 'age_level'],\n",
      "      dtype='object')\n",
      "CPU times: user 18.9 ms, sys: 2.32 ms, total: 21.2 ms\n",
      "Wall time: 24 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 合并账户静态信息\n",
    "static_info['khrq']  = pd.to_datetime(static_info['khrq'], format='%Y-%m-%d')\n",
    "static_info['year']  = static_info['khrq'].dt.year\n",
    "static_info['month'] = static_info['khrq'].dt.month\n",
    "static_info['day']   = static_info['khrq'].dt.day\n",
    "\n",
    "# 自然数编码\n",
    "def label_encode(series):\n",
    "    unique = list(series.unique())\n",
    "    return series.map(dict(zip(\n",
    "        unique, range(series.nunique())\n",
    "    )))\n",
    "\n",
    "for col in ['khjgdh']:\n",
    "    static_info[col] = label_encode(static_info[col])\n",
    "    \n",
    "static_info['age_level'] = static_info['年龄'] // 5\n",
    "keep_cols = ['zhdh','year','month','day','khjgdh','xb','年龄', 'khrq', 'age_level']\n",
    "\n",
    "train_set = train_set.merge(static_info[keep_cols], on=['zhdh'], how='left')\n",
    "test_set  = test_set.merge(static_info[keep_cols], on=['zhdh'], how='left')\n",
    "\n",
    "basic_fea = list(train_set.columns)\n",
    "print(train_set.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8c1f959f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time col -  Index(['jylsxh', 'zhdh', 'dfzh', 'jdbj', 'jyje', 'zhye', 'dfhh', 'jyrq',\n",
      "       'jysj', 'jyqd', 'zydh', 'dfmccd', 'date', 'session_id'],\n",
      "      dtype='object')\n",
      "CPU times: user 3.42 s, sys: 220 ms, total: 3.64 s\n",
      "Wall time: 3.65 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 静态时间特征\n",
    "def basic_time_fea(df):\n",
    "    col = 'date'\n",
    "    df[col] = pd.to_datetime(df['jyrq'] + ' ' + df['jysj'])\n",
    "    df['jyrq_year'] = df[col].dt.year\n",
    "    df['jyrq_month'] = df[col].dt.month\n",
    "    df['jyrq_day'] = df[col].dt.day\n",
    "    df['jyrq_hour'] = df[col].dt.hour\n",
    "    \n",
    "    df['jyrq_weekofyear'] = df[col].dt.weekofyear\n",
    "    df['jyrq_dayofyear'] = df[col].dt.dayofyear\n",
    "    \n",
    "\n",
    "basic_time_fea(trans_info)\n",
    "trans_info = trans_info.sort_values(by='date')\n",
    "\n",
    "col = ['jylsxh', 'zhdh', 'dfzh', 'jdbj', 'jyje', 'zhye', 'dfhh', 'jyrq','jysj', 'jyqd', 'zydh', 'dfmccd']\n",
    "trans_info = trans_info.merge(session[col + ['session_id']], how='left', on=col)\n",
    "print('time col - ', session.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "b92b10ab",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.32 s, sys: 78.3 ms, total: 1.4 s\n",
      "Wall time: 1.41 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 转入转出次数、金额比例、dfzh个数\n",
    "in_count = trans_info[trans_info['jdbj'] == 1].groupby('zhdh').size().reset_index().rename(columns={0: 'in_count'})\n",
    "out_count = trans_info[trans_info['jdbj'] == 0].groupby('zhdh').size().reset_index().rename(columns={0: 'out_count'})\n",
    "\n",
    "in_sum = trans_info[trans_info['jdbj'] == 1].groupby('zhdh')['jyje'].sum().reset_index().rename(columns={'jyje': 'in_sum'})\n",
    "out_sum = trans_info[trans_info['jdbj'] == 0].groupby('zhdh')['jyje'].sum().reset_index().rename(columns={'jyje': 'out_sum'})\n",
    "\n",
    "in_uniq = trans_info[trans_info['jdbj'] == 1].groupby('zhdh')['dfzh'].\\\n",
    "                            nunique().reset_index().rename(columns={'dfzh': 'in_uniq'})\n",
    "out_uniq = trans_info[trans_info['jdbj'] == 0].groupby('zhdh')['dfzh'].\\\n",
    "                            nunique().reset_index().rename(columns={'dfzh': 'out_uniq'})\n",
    "\n",
    "\n",
    "in_abs_uniq = trans_info[trans_info['jdbj'] == 1].groupby('zhdh')['zydh'].\\\n",
    "                            nunique().reset_index().rename(columns={'zydh': 'in_abs_uniq'})\n",
    "out_abs_uniq = trans_info[trans_info['jdbj'] == 0].groupby('zhdh')['zydh'].\\\n",
    "                            nunique().reset_index().rename(columns={'zydh': 'out_abs_uniq'})\n",
    "\n",
    "\n",
    "for tmp in [in_count, out_count, in_sum, out_sum, in_uniq, out_uniq, in_abs_uniq, out_abs_uniq]:\n",
    "    train_set = train_set.merge(tmp, how='left', on='zhdh')\n",
    "    test_set = test_set.merge(tmp, how='left', on='zhdh')\n",
    "    \n",
    "train_set['trans_count_ration'] = train_set['in_count'] / train_set['out_count']\n",
    "test_set['trans_count_ration'] = test_set['in_count'] / test_set['out_count']\n",
    "\n",
    "train_set['trans_sum_ration'] = train_set['in_sum'] / train_set['out_sum']\n",
    "test_set['trans_sum_ration'] = test_set['in_sum'] / test_set['out_sum']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fb043975",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 480 ms, sys: 7.33 ms, total: 488 ms\n",
      "Wall time: 486 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 每小时交易/转出超过3，5，8，10 次的用户\n",
    "trans_out = trans_info[trans_info['jdbj'] == 0].groupby(['zhdh', 'jyrq_dayofyear', 'jyrq_hour']).size().reset_index()\n",
    "trans_all = trans_info.groupby(['zhdh', 'jyrq_dayofyear', 'jyrq_hour']).size().reset_index()\n",
    "\n",
    "for num in [3, 5, 8, 10]:\n",
    "    tmp1 = trans_out[trans_out[0] > num]['zhdh'].unique()\n",
    "    tmp2 = trans_all[trans_all[0] > num]['zhdh'].unique()\n",
    "    \n",
    "    train_set['trans_out_count_{}'.format(num)], test_set['trans_out_count_{}'.format(num)] = 0, 0\n",
    "    train_set.loc[train_set['zhdh'].isin(tmp1), 'trans_out_count_{}'.format(num)] = 1\n",
    "    test_set.loc[test_set['zhdh'].isin(tmp1), 'trans_out_count_{}'.format(num)] = 1\n",
    "\n",
    "    train_set['trans_all_count_{}'.format(num)], test_set['trans_all_count_{}'.format(num)] = 0, 0\n",
    "    train_set.loc[train_set['zhdh'].isin(tmp2), 'trans_all_count_{}'.format(num)] = 1\n",
    "    test_set.loc[test_set['zhdh'].isin(tmp2), 'trans_all_count_{}'.format(num)] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "53834382",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 517 ms, sys: 17.4 ms, total: 535 ms\n",
      "Wall time: 534 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 转入转出抵消\n",
    "trans_info.loc[trans_info['jdbj'] == 0, 'jdbj'] = -1\n",
    "trans_info['balance'] = trans_info['jdbj'] * trans_info['jyje']\n",
    "\n",
    "def count_zero(x):\n",
    "    count = 0\n",
    "    tag = 0\n",
    "    record = []\n",
    "    for i in x:\n",
    "        if i > 0 and tag == 0:\n",
    "            tag += 1\n",
    "        if i < 0 and tag == 1:\n",
    "            tag += 1\n",
    "            \n",
    "        if tag == 2 and i > 0:\n",
    "\n",
    "            if sum(record) == 0:\n",
    "                count += 1\n",
    "            tag = 0\n",
    "            record = []\n",
    "        record.append(i)\n",
    "    return count\n",
    "        \n",
    "\n",
    "user_info = trans_info.groupby('zhdh')['balance'].apply(list).reset_index()\n",
    "user_info['zero_count'] = user_info['balance'].map(count_zero)\n",
    "del user_info['balance']\n",
    "\n",
    "train_set = train_set.merge(user_info, how='left', on='zhdh')\n",
    "test_set = test_set.merge(user_info, how='left', on='zhdh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "eaf11f12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 33.3 s, sys: 158 ms, total: 33.5 s\n",
      "Wall time: 26.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# word2vec 特征\n",
    "tmp = trans_info.groupby('zhdh')['balance'].apply(list).reset_index()\n",
    "tmp['sent'] = tmp['balance'].apply(lambda x: [str(i) for i in x])\n",
    "\n",
    "sentence = []\n",
    "for line in tmp['sent'].tolist():\n",
    "    sentence.append([str(float(l)) for idx, l in enumerate(line)])\n",
    "\n",
    "vs = 10\n",
    "model = Word2Vec(sentence, vector_size=vs, window=2, min_count=1, workers=7)\n",
    "new_col = ['word2vec_{}'.format(i) for i in range(vs)]\n",
    "for i in range(vs):\n",
    "    tmp['word2vec_{}'.format(i)] = tmp['balance'].apply(lambda x: sum([model.wv[str(i)] for i in x])[i])\n",
    "\n",
    "\n",
    "train_set = train_set.merge(tmp[['zhdh'] + new_col], how='left', on='zhdh')\n",
    "test_set = test_set.merge(tmp[['zhdh'] + new_col], how='left', on='zhdh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5c1eb3ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 17.8 s, sys: 157 ms, total: 17.9 s\n",
      "Wall time: 18.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 远程控制app转账 - 每小时欺诈概率(前一小时，当前小时，后一小时的欺诈概率)\n",
    "trans_info['hour'] = trans_info['jyrq_dayofyear'] * 24 + trans_info['jyrq_hour']\n",
    "\n",
    "timeline = trans_info['hour'].unique()\n",
    "ts_prob = dict()\n",
    "for ts in timeline[1:-1]:\n",
    "    li = [ts - 1, ts, ts + 1]\n",
    "    tmp = trans_info[trans_info['hour'].isin(li)]['zhdh'].unique()\n",
    "    ts_prob[ts] = train_set[train_set['zhdh'].isin(tmp)]['black_flag'].mean()\n",
    "    \n",
    "trans_info['hour_pro'] = trans_info['hour'].map(ts_prob)\n",
    "hour_prob = trans_info.groupby('zhdh')['hour_pro'].mean().reset_index()\n",
    "\n",
    "train_set = train_set.merge(hour_prob, how='left', on='zhdh')\n",
    "test_set = test_set.merge(hour_prob, how='left', on='zhdh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7a727922",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1.05 s, sys: 25.1 ms, total: 1.07 s\n",
      "Wall time: 1.07 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 账户存活时间短\n",
    "trans_diff = trans_info.groupby('zhdh').agg({'jyrq': ['max', 'min']}).reset_index()\n",
    "trans_diff.columns = ['zhdh', 'jyrq_max', 'jyrq_min']\n",
    "trans_diff['trans_duration'] = (pd.to_datetime(trans_diff['jyrq_max']) - \\\n",
    "                                                pd.to_datetime(trans_diff['jyrq_min'])).dt.days\n",
    "\n",
    "train_set = train_set.merge(trans_diff[['zhdh', 'trans_duration']], how='left', on='zhdh')\n",
    "test_set = test_set.merge(trans_diff[['zhdh', 'trans_duration']], how='left', on='zhdh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b54be503",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 216 ms, sys: 6.98 ms, total: 223 ms\n",
      "Wall time: 222 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 天数少\n",
    "day_count = trans_info.groupby('zhdh')['jyrq'].nunique().reset_index()\n",
    "day_count.columns = ['zhdh', 'trans_day_count']\n",
    "\n",
    "train_set = train_set.merge(day_count, how='left', on='zhdh')\n",
    "test_set = test_set.merge(day_count, how='left', on='zhdh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9c9f66f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 235 ms, sys: 39.3 ms, total: 274 ms\n",
      "Wall time: 276 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# 每天余额情况\n",
    "day_rest_mean = trans_info.drop_duplicates(subset=['zhdh', 'jyrq'], keep='last').\\\n",
    "                            groupby('zhdh')['zhye'].mean().reset_index()\n",
    "day_rest_mean.columns = ['zhdh', 'day_rest_mean'] \n",
    "\n",
    "train_set = train_set.merge(day_rest_mean, how='left', on='zhdh')\n",
    "test_set = test_set.merge(day_rest_mean, how='left', on='zhdh')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8da78c77",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# # 趋势特征\n",
    "# # 每周的转账次数情况、dfzh、dfhh等情况（算比例？）\n",
    "# week_num = trans_info['jyrq_weekofyear'].unique()\n",
    "# print(week_num)\n",
    "# for idx in range(0, len(week_num), 2):\n",
    "#     week_count = trans_info[trans_info['jyrq_weekofyear'].\\\n",
    "#                          isin([week_num[idx], week_num[idx + 1]])].groupby('zhdh').size().reset_index()\n",
    "#     week_count.columns = ['zhdh', 'trans_count_of_week{}'.format(week_num[idx])]\n",
    "    \n",
    "#     train_set = train_set.merge(week_count, how='left', on='zhdh')\n",
    "#     test_set = test_set.merge(week_count, how='left', on='zhdh')\n",
    "    \n",
    "#     week_sum = trans_info[trans_info['jyrq_weekofyear'].\\\n",
    "#                          isin([week_num[idx], week_num[idx + 1]])].groupby('zhdh')['jyje'].sum().reset_index()\n",
    "#     week_sum.columns = ['zhdh', 'trans_sum_of_week{}'.format(week_num[idx])]\n",
    "    \n",
    "#     train_set = train_set.merge(week_sum, how='left', on='zhdh')\n",
    "#     test_set = test_set.merge(week_sum, how='left', on='zhdh')\n",
    "\n",
    "\n",
    "# for idx in range(2, len(week_num), 2):\n",
    "#     train_set['count_ration_of_week_{}'.format(idx)] = train_set['trans_count_of_week{}'.format(week_num[idx])] / \\\n",
    "#                                                         train_set['trans_count_of_week{}'.format(week_num[idx - 2])]\n",
    "#     test_set['count_ration_of_week_{}'.format(idx)] = test_set['trans_count_of_week{}'.format(week_num[idx])] / \\\n",
    "#                                                         test_set['trans_count_of_week{}'.format(week_num[idx - 2])]\n",
    "    \n",
    "#     train_set['sum_ration_of_week_{}'.format(idx)] = train_set['trans_sum_of_week{}'.format(week_num[idx])] / \\\n",
    "#                                                         train_set['trans_sum_of_week{}'.format(week_num[idx - 2])]\n",
    "#     test_set['sum_ration_of_week_{}'.format(idx)] = test_set['trans_sum_of_week{}'.format(week_num[idx])] / \\\n",
    "#                                                         test_set['trans_sum_of_week{}'.format(week_num[idx - 2])]\n",
    "\n",
    "# 每周特征\n",
    "week_num = trans_info['jyrq_weekofyear'].unique()\n",
    "agg_func = {\n",
    "    'jyje': ['count', 'sum'],\n",
    "    'dfzh': ['nunique'],\n",
    "    'dfhh': ['nunique'],\n",
    "    'jyqd': ['nunique']\n",
    "}\n",
    "new_col = ['{}_{}_of_week'.format(k, i) for k, v in agg_func.items() for i in v]\n",
    "\n",
    "for week in week_num:\n",
    "    \n",
    "    agg_df = trans_info[trans_info['jyrq_weekofyear'] == week].groupby('zhdh').agg(agg_func).reset_index()\n",
    "    agg_df.columns = ['zhdh'] + [i + str(week) for i in new_col]\n",
    "    \n",
    "    train_set = train_set.merge(agg_df, how='left', on='zhdh')\n",
    "    test_set = test_set.merge(agg_df, how='left', on='zhdh')\n",
    "    \n",
    "    \n",
    "for week in week_num[1:]:\n",
    "    for col in new_col:\n",
    "        # 差分特征\n",
    "        train_set['diff_{}_{}_{}'.format(week, week - 1, col)] = \\\n",
    "                train_set[col + str(week)] - train_set[col + str(week - 1)]\n",
    "        test_set['diff_{}_{}_{}'.format(week, week - 1, col)] = \\\n",
    "                test_set[col + str(week)] - test_set[col + str(week - 1)]\n",
    "        \n",
    "        # huanbi\n",
    "        train_set['ration_{}_{}_{}'.format(week, week - 1, col)] = \\\n",
    "                train_set[col + str(week)] / train_set[col + str(week - 1)]\n",
    "        test_set['ration_{}_{}_{}'.format(week, week - 1, col)] = \\\n",
    "                test_set[col + str(week)] / test_set[col + str(week - 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ebfe0f6c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def cv_model(clf, train_x, train_y, test_x, clf_name):\n",
    "    folds = 5\n",
    "    seed = 2023\n",
    "    kf = KFold(n_splits=folds, shuffle=True, random_state=seed)\n",
    "    \n",
    "    oof = np.zeros(train_x.shape[0])\n",
    "    predict = np.zeros(test_x.shape[0])\n",
    "\n",
    "    cv_scores = []\n",
    "    importance = []\n",
    "    for i, (train_index, valid_index) in enumerate(kf.split(train_x, train_y)):\n",
    "        print('************************************ {} ************************************'.format(str(i+1)))\n",
    "        trn_x, trn_y, val_x, val_y = \\\n",
    "                train_x.iloc[train_index], train_y[train_index], train_x.iloc[valid_index], train_y[valid_index]\n",
    "\n",
    "        if clf_name == \"lgb\":\n",
    "            train_matrix = clf.Dataset(trn_x, label=trn_y)\n",
    "            valid_matrix = clf.Dataset(val_x, label=val_y)\n",
    "\n",
    "            params = {\n",
    "                'boosting_type': 'gbdt',\n",
    "                'objective': 'binary',\n",
    "                'metric': 'auc',\n",
    "                'min_child_weight': 5,\n",
    "                'num_leaves': 2 ** 5,\n",
    "                'lambda_l2': 10,\n",
    "                'feature_fraction': 0.8,\n",
    "                'bagging_fraction': 0.8,\n",
    "                'bagging_freq': 4,\n",
    "                'learning_rate': 0.01,\n",
    "                'seed': 2020,\n",
    "                'n_jobs':8\n",
    "            }\n",
    "\n",
    "            model = clf.train(params, train_matrix, 10000, valid_sets=[train_matrix, valid_matrix], \n",
    "                              categorical_feature=[], verbose_eval=200, early_stopping_rounds=200)\n",
    "            val_pred = model.predict(val_x, num_iteration=model.best_iteration)\n",
    "            test_pred = model.predict(test_x, num_iteration=model.best_iteration)\n",
    "            \n",
    "            print(list(sorted(zip(features, \\\n",
    "                                  model.feature_importance(\"gain\")), key=lambda x: x[1], reverse=True))[:20])\n",
    "                \n",
    "        if clf_name == \"xgb\":\n",
    "            train_matrix = clf.DMatrix(trn_x , label=trn_y)\n",
    "            valid_matrix = clf.DMatrix(val_x , label=val_y)\n",
    "            test_matrix = clf.DMatrix(test_x)\n",
    "            \n",
    "            params = {'booster': 'gbtree',\n",
    "                      'objective': 'binary:logistic',\n",
    "                      'eval_metric': 'auc',\n",
    "                      'gamma': 1,\n",
    "                      'min_child_weight': 1.5,\n",
    "                      'max_depth': 5,\n",
    "                      'lambda': 10,\n",
    "                      'subsample': 0.7,\n",
    "                      'colsample_bytree': 0.7,\n",
    "                      'colsample_bylevel': 0.7,\n",
    "                      'eta': 0.05,\n",
    "                      'tree_method': 'exact',\n",
    "                      'seed': 2020,\n",
    "                      'nthread': 8\n",
    "                      }\n",
    "            \n",
    "            watchlist = [(train_matrix, 'train'),(valid_matrix, 'eval')]\n",
    "            \n",
    "            model = clf.train(params, train_matrix, num_boost_round=10000,\n",
    "                              evals=watchlist, verbose_eval=1000, early_stopping_rounds=500)\n",
    "            \n",
    "            val_pred  = model.predict(valid_matrix, ntree_limit=model.best_ntree_limit)\n",
    "            test_pred = model.predict(test_matrix , ntree_limit=model.best_ntree_limit)\n",
    "                 \n",
    "        if clf_name == \"cat\":\n",
    "            \n",
    "            model = clf(\n",
    "                        n_estimators=10000,\n",
    "                        random_seed=1024,\n",
    "                        eval_metric='AUC',\n",
    "                        learning_rate=0.05,\n",
    "                        max_depth=5,\n",
    "                        early_stopping_rounds=200,\n",
    "                        metric_period=500,\n",
    "                    )\n",
    "\n",
    "            model.fit(trn_x, trn_y, eval_set=(val_x, val_y),\n",
    "                      use_best_model=True,\n",
    "                      verbose=1)\n",
    "            \n",
    "            val_pred  = model.predict_proba(val_x)[:,1]\n",
    "            test_pred = model.predict_proba(test_x)[:,1]\n",
    "            \n",
    "        oof[valid_index] = val_pred\n",
    "        predict += test_pred / kf.n_splits\n",
    "        \n",
    "        cv_scores.append(roc_auc_score(val_y, val_pred))\n",
    "        print(cv_scores)\n",
    "        importance += [model.get_fscore()]\n",
    "       \n",
    "    return oof, predict, importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "6d981556",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "************************************ 1 ************************************\n",
      "[0]\ttrain-auc:0.90593\teval-auc:0.83537\n",
      "[1000]\ttrain-auc:0.99980\teval-auc:0.93361\n",
      "[1058]\ttrain-auc:0.99982\teval-auc:0.93398\n",
      "[0.9339814814814814]\n",
      "************************************ 2 ************************************\n",
      "[0]\ttrain-auc:0.90536\teval-auc:0.81873\n",
      "[632]\ttrain-auc:0.99955\teval-auc:0.87749\n",
      "[0.9339814814814814, 0.8880028996013049]\n",
      "************************************ 3 ************************************\n",
      "[0]\ttrain-auc:0.89243\teval-auc:0.84555\n",
      "[778]\ttrain-auc:0.99975\teval-auc:0.90172\n",
      "[0.9339814814814814, 0.8880028996013049, 0.9085012285012285]\n",
      "************************************ 4 ************************************\n",
      "[0]\ttrain-auc:0.86797\teval-auc:0.89943\n",
      "[939]\ttrain-auc:0.99974\teval-auc:0.95193\n",
      "[0.9339814814814814, 0.8880028996013049, 0.9085012285012285, 0.9536364451618689]\n",
      "************************************ 5 ************************************\n",
      "[0]\ttrain-auc:0.88828\teval-auc:0.82806\n",
      "[679]\ttrain-auc:0.99992\teval-auc:0.93296\n",
      "[0.9339814814814814, 0.8880028996013049, 0.9085012285012285, 0.9536364451618689, 0.9345370370370369]\n",
      "************************************ 1 ************************************\n",
      "[0]\ttrain-auc:0.91957\teval-auc:0.91306\n",
      "[579]\ttrain-auc:0.99747\teval-auc:0.96500\n",
      "[0.969537037037037]\n",
      "************************************ 2 ************************************\n",
      "[0]\ttrain-auc:0.92423\teval-auc:0.86281\n",
      "[1000]\ttrain-auc:0.99845\teval-auc:0.96167\n",
      "[1335]\ttrain-auc:0.99876\teval-auc:0.96240\n",
      "[0.969537037037037, 0.9628488582819862]\n",
      "************************************ 3 ************************************\n",
      "[0]\ttrain-auc:0.91122\teval-auc:0.87966\n",
      "[924]\ttrain-auc:0.99872\teval-auc:0.96668\n",
      "[0.969537037037037, 0.9628488582819862, 0.967960687960688]\n",
      "************************************ 4 ************************************\n",
      "[0]\ttrain-auc:0.88863\teval-auc:0.90633\n",
      "[704]\ttrain-auc:0.99727\teval-auc:0.98323\n",
      "[0.969537037037037, 0.9628488582819862, 0.967960687960688, 0.9858308671867995]\n",
      "************************************ 5 ************************************\n",
      "[0]\ttrain-auc:0.92048\teval-auc:0.85972\n",
      "[711]\ttrain-auc:0.99876\teval-auc:0.93796\n",
      "[0.969537037037037, 0.9628488582819862, 0.967960687960688, 0.9858308671867995, 0.9442592592592592]\n",
      "threshold - 0.49, best f1 - 0.9035974965953084\n"
     ]
    }
   ],
   "source": [
    "ts_fea = [i for i in train_set.columns if 'of_week' in i]\n",
    "ts_cols = [i for i in basic_fea + [i for i in train_set.columns if 'of_week' in i] if i != 'black_flag']\n",
    "ts_cols = [f for f in ts_cols if f not in ['zhdh','black_flag', 'khrq']]\n",
    "\n",
    "\n",
    "xgb_oof, xgb_pred, xgb_importance = cv_model(xgb, train_set[ts_cols], train_set['black_flag'], test_set[ts_cols], 'xgb')\n",
    "\n",
    "train_set['ts_prob'] = xgb_oof\n",
    "test_set['ts_prob'] = xgb_pred\n",
    "\n",
    "cols = [f for f in train_set.columns if f not in ['zhdh','black_flag', 'khrq'] + ts_fea]\n",
    "xgb_oof, xgb_pred, xgb_importance = cv_model(xgb, train_set[cols], train_set['black_flag'], test_set[cols], 'xgb')\n",
    "\n",
    "\n",
    "oof = xgb_oof\n",
    "scores = []; thresholds = []\n",
    "best_score = 0; best_threshold = 0\n",
    "\n",
    "for threshold in np.arange(0.4, 0.6, 0.01):\n",
    "    preds = (oof.reshape((-1)) > threshold).astype('int')\n",
    "    m = f1_score(train_set['black_flag'].values.reshape((-1)), preds, average='macro')   \n",
    "    scores.append(m)\n",
    "    thresholds.append(threshold)\n",
    "    if m > best_score:\n",
    "        best_score = m\n",
    "        best_threshold = threshold\n",
    "print(f'threshold - {best_threshold:.02f}, best f1 - {best_score}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "79a4fe25",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "in_uniq 1022\n",
      "ts_prob 719\n",
      "hour_pro 681\n",
      "day_rest_mean 629\n",
      "word2vec_3 544\n",
      "trans_duration 513\n",
      "trans_day_count 502\n",
      "word2vec_0 432\n",
      "year 419\n",
      "word2vec_1 385\n",
      "年龄 331\n",
      "trans_sum_ration 318\n",
      "in_count 317\n",
      "trans_count_ration 315\n",
      "khjgdh 299\n",
      "out_sum 260\n",
      "day 254\n",
      "in_sum 227\n",
      "out_abs_uniq 224\n",
      "word2vec_9 212\n",
      "out_uniq 205\n",
      "out_count 198\n",
      "trans_out_count_5 167\n",
      "month 149\n",
      "word2vec_8 148\n",
      "word2vec_4 148\n",
      "word2vec_2 125\n",
      "xb 101\n",
      "word2vec_5 86\n",
      "zero_count 85\n",
      "trans_all_count_5 73\n",
      "word2vec_6 71\n",
      "word2vec_7 69\n",
      "in_abs_uniq 64\n",
      "age_level 58\n",
      "trans_all_count_8 54\n",
      "trans_out_count_3 29\n",
      "trans_all_count_10 26\n",
      "trans_all_count_3 16\n",
      "trans_out_count_10 5\n",
      "trans_out_count_8 2\n"
     ]
    }
   ],
   "source": [
    "base = xgb_importance[0]\n",
    "\n",
    "for sub in xgb_importance[1:]:\n",
    "    for k, v in sub.items():\n",
    "        base[k] = base.get(k, 0) + v\n",
    "\n",
    "for k, v in sorted(base.items(), key=lambda x: x[1], reverse=True):\n",
    "    print(k, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6221d187",
   "metadata": {},
   "outputs": [],
   "source": [
    "# threshold - 0.44, best f1 - 0.7102132901597491\n",
    "# threshold - 0.41, best f1 - 0.8506426890350612 - 0.75757575758 \n",
    "# threshold - 0.51, best f1 - 0.8753058535667231 - 0.78381642512 \n",
    "# threshold - 0.49, best f1 - 0.8732163783502427 - 0.78700361011 \n",
    "# threshold - 0.47, best f1 - 0.8784692917942659 - 0.78871548619 \n",
    "# threshold - 0.47, best f1 - 0.8787658362126447 -  0.79302045728 \n",
    "# threshold - 0.45, best f1 - 0.8987062281443039 - 0.80263947211 hour_prob(nice呀)\n",
    "# threshold - 0.46, best f1 - 0.9077434036533613 - ？还没尝试 应该nice  trans_duration\n",
    "# threshold - 0.40, best f1 - 0.9117434071570816 - ？还没尝试 也应该nice  trans_day_count\n",
    "# threshold - 0.40, best f1 - 0.9148403872302214 - 0.85645355850  day_rest_mean \n",
    "# threshold - 0.47, best f1 - 0.9049738098549112 - 0.86499402628  word2vec\n",
    "# threshold - 0.51, best f1 - 0.9043998160944254 - 0.87617924528 ts_prob\n",
    "# threshold - 0.49, best f1 - 0.9035974965953084 -  0.87773183698 ts_prob (修改特征)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3b5a9b91",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.22208333333333333, 0.25)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pred = xgb_pred\n",
    "\n",
    "train_set['oof'] = oof\n",
    "# +++++++++++++++++++\n",
    "best_threshold = 0.5\n",
    "# +++++++++++++++++++\n",
    "train_set['y_hat'] = train_set['oof'].apply(lambda x: 1 if x > best_threshold else 0)\n",
    "train_set['error'] = train_set.apply(lambda x: abs(x['black_flag'] - x['oof']), axis=1)\n",
    "\n",
    "\n",
    "test_set['black_flag'] = (pred.reshape((-1)) > best_threshold).astype(int)\n",
    "\n",
    "test_set[['zhdh','black_flag']].to_csv('submission.csv', index=False)\n",
    "test_set['black_flag'].mean(), train_set['black_flag'].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "77010244",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "head = train_set[train_set['error'] > 0.5].\\\n",
    "                sort_values(by='error')[['zhdh', 'error', 'oof', 'y_hat', 'black_flag']].head(30)\n",
    "\n",
    "\n",
    "with pd.ExcelWriter('error.xlsx') as writer:  \n",
    "    trans_info[trans_info['zhdh'].isin(train_set[train_set['black_flag'] == 1]['zhdh'].unique())].\\\n",
    "                                        to_excel(writer, sheet_name='black', index=None)\n",
    "    static_info[static_info['zhdh'].isin(train_set[train_set['black_flag'] == 1]['zhdh'].unique())][keep_cols].\\\n",
    "                                        to_excel(writer, sheet_name='black_static', index=None)\n",
    "#     trans_info[trans_info['zhdh'].isin(train_set[train_set['black_flag'] == 0]['zhdh'])].\\\n",
    "#                                         to_excel(writer, sheet_name='white', index=None)\n",
    "#     static_info[static_info['zhdh'].isin(train_set[train_set['black_flag'] == 0]['zhdh'])][keep_cols].\\\n",
    "#                                         to_excel(writer, sheet_name='white_static', index=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7dab349e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b3e874d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79e34a33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "raw",
   "id": "4949bfaf",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
